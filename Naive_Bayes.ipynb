{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS91NbJiXnST"
      },
      "source": [
        "# **<u>Naive Bayes Sentence Classifier</u>**\n",
        "**Class**: Northwestern CS 349 Fall 2024<br>\n",
        "**Professor**: David Demeter<br>\n",
        "**Contributers**: Raymond Gu, Mimi Zhang, Alvin Xu, Rhema Phiri, Eshan Haq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuVSUE70Z8e2"
      },
      "source": [
        "## **<u>Import Libraries & Modules</u>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_zU8AF1aLwt",
        "outputId": "b22558cb-8e0e-4a70-be38-fa619dd27006"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import Pandas Module\n",
        "import pandas as pd\n",
        "\n",
        "# Import SKLearn\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Import NLTK (Parsing Text)\n",
        "import nltk\n",
        "\n",
        "# Import Numpy Module\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx2IMwJ7WV5x"
      },
      "source": [
        "## **<u>Creating Train, Validation, and Test Subsets</u>**\n",
        "The original dataset is labelled `sentences.csv` and this function produces `train.csv`, `valid.csv`, and `test.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ute-eBwCWA7v"
      },
      "outputs": [],
      "source": [
        "def get_datasets(dataset_name):\n",
        "\n",
        "    # Get the paths of each of the datasets\n",
        "    dataset = dataset_name + \".csv\"\n",
        "\n",
        "    df = pd.read_csv(dataset)\n",
        "\n",
        "    # Drop Kaggle Index Column\n",
        "    df = df.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "    # Removing duplicates\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    df.columns = ['Sentence', 'Label']\n",
        "\n",
        "    # Ensuring there is even distribution of labels\n",
        "    sample_size = 37500\n",
        "    num_labels = df['Label'].nunique()\n",
        "    samples_per_label = sample_size // num_labels\n",
        "    even_df = pd.concat([resample(group, n_samples=samples_per_label, random_state=42) for _, group in df.groupby('Label')])\n",
        "\n",
        "    even_df = even_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    X = even_df['Sentence']\n",
        "    y = even_df['Label']\n",
        "\n",
        "    # Splitting dataset\n",
        "    X_train, X_other, y_train, y_other = train_test_split( X,y , random_state=104,test_size=0.2, stratify=y)\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, random_state=42,test_size=0.5, stratify=y_other)\n",
        "\n",
        "    # Convert to Dataframe and then convert to csv files\n",
        "    train_data = {'Label' : y_train, 'Sentence' : X_train}\n",
        "    train_data = pd.DataFrame(train_data)\n",
        "    train_data.to_csv('train.csv', index=False)\n",
        "\n",
        "    test_data = {'Label' : y_test, 'Sentence' : X_test}\n",
        "    test_data = pd.DataFrame(test_data)\n",
        "    test_data.to_csv('test.csv', index=False)\n",
        "\n",
        "    valid_data = {'Label' : y_val, 'Sentence' : X_val}\n",
        "    valid_data = pd.DataFrame(valid_data)\n",
        "    valid_data.to_csv('valid.csv', index=False)\n",
        "\n",
        "    return None\n",
        "\n",
        "get_datasets('sentences')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8GWzC8GjBJQ"
      },
      "source": [
        "## **<u>Naive Bayes Classifier Code</u>**\n",
        "This section contains all the code for our Naive Bayes Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeGf6N-hMMHI"
      },
      "source": [
        "> ### <u>Section 1: Naive Bayes Class</u>\n",
        "This code defines the structure of our Naive Bayes classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1SbShffYMNeE"
      },
      "outputs": [],
      "source": [
        "class Naive_Bayes():\n",
        "\n",
        "  # Initializer\n",
        "  # --------------------------------------------------------------------------------\n",
        "  def __init__(self):\n",
        "      # Create a dictionary that maps label to the classification\n",
        "      self.emotionLabels = {0: 'sadness',\n",
        "                            1: 'joy',\n",
        "                            2: 'love',\n",
        "                            3: 'anger',\n",
        "                            4: 'fear',\n",
        "                            5: 'surprise'}\n",
        "\n",
        "      # Initialize TF-IDF Vectorizer to be trained\n",
        "      self.vectorizer = TfidfVectorizer()\n",
        "\n",
        "      # Class attribute to store probability of each word given the class\n",
        "      self.word_probs = None\n",
        "\n",
        "      # Default Additive Smoothing parameter (so no probability is 0)\n",
        "      self.alpha = 1\n",
        "\n",
        "  # Train Function\n",
        "  # --------------------------------------------------------------------------------\n",
        "  def train(self, train, valid):\n",
        "      '''\n",
        "      Purpose: Trains the model using BOTH the training data and validation data.\n",
        "\n",
        "      Arguments:\n",
        "      - Train: The path to the training data.\n",
        "      - Valid: The path to the validation data.\n",
        "      '''\n",
        "      # Read the training data and load the sentences into a dataframe\n",
        "      train_data = pd.read_csv(train)\n",
        "      train_data.columns = ['Label', 'Sentence']\n",
        "      train_sentences = train_data['Sentence'].tolist()\n",
        "\n",
        "      # Read the validation data and load the sentences into a dataframe\n",
        "      valid_data = pd.read_csv(valid)\n",
        "      valid_data.columns = ['Label', 'Sentence']\n",
        "      valid_sentences = valid_data['Sentence'].tolist()\n",
        "\n",
        "      # Combine all sentences and fit the vectorizer\n",
        "      corpus = train_sentences + valid_sentences\n",
        "      self.vectorizer.fit(corpus)\n",
        "\n",
        "      # Transform train and valid sentences using the fitted vectorizer\n",
        "      x_train = self.vectorizer.transform(train_sentences).toarray()\n",
        "      x_valid = self.vectorizer.transform(valid_sentences).toarray()\n",
        "\n",
        "      # Extract the labels from the training and validation dataframes\n",
        "      y_train = train_data['Label'].values\n",
        "      y_valid = valid_data['Label'].values\n",
        "\n",
        "      # Tune the value of our Additive smoothing parameter\n",
        "      best_alpha = 1\n",
        "      best_accuracy = 0\n",
        "      for alpha in np.linspace(0.02, 1, 50):\n",
        "          self._fit(x_train, y_train, alpha)\n",
        "          accuracy = self._validate(x_valid, y_valid)\n",
        "          if accuracy > best_accuracy:\n",
        "              best_accuracy = accuracy\n",
        "              best_alpha = alpha\n",
        "\n",
        "      # Train the final model with the best alpha\n",
        "      self._fit(x_train, y_train, best_alpha)\n",
        "\n",
        "  # Fit Function\n",
        "  # --------------------------------------------------------------------------------\n",
        "  def _fit(self, x_train, y_train, alpha):\n",
        "      '''\n",
        "      Purpose: This function helps the model learn conditional probabilities by\n",
        "               calculating the probability of each word occuring in each class.\n",
        "\n",
        "      Note: The smoothing parameter (alpha) is used to handle the issue of zero\n",
        "            probabilities. This issue can happen if a word does not occur in the\n",
        "            training dataset.\n",
        "\n",
        "      Arguments:\n",
        "      - X_Train: The training data.\n",
        "      - Y_Train: The labels for the training data.\n",
        "      - Alpha: The Laplace smoothing parameter.\n",
        "      '''\n",
        "      # Set the Laplace Smoothing constant to the inputted argument\n",
        "      self.alpha = alpha\n",
        "      n_classes = 6\n",
        "      n_features = x_train.shape[1]\n",
        "\n",
        "      # Initialize word probabilities for each class\n",
        "      self.word_probs = np.zeros((n_classes, n_features))\n",
        "\n",
        "      # Compute word probabilities for each class\n",
        "      for c in range(n_classes):\n",
        "          class_indices = (y_train == c)\n",
        "          class_data = x_train[class_indices]\n",
        "\n",
        "          # Get the total number occurances a word occurs in the class\n",
        "          total_occurances_word = np.sum(class_data, axis=0)\n",
        "\n",
        "          # Get the total number of words that occur in the class\n",
        "          total_words_class = np.sum(class_data)\n",
        "\n",
        "          # Calculate the smooth probability for the word\n",
        "          numerator = total_occurances_word + alpha\n",
        "          denominator = total_words_class + alpha * n_features\n",
        "          self.word_probs[c, :] = numerator / denominator\n",
        "\n",
        "  # Validate Function\n",
        "  # --------------------------------------------------------------------------------\n",
        "  def _validate(self, x_valid, y_valid):\n",
        "      '''\n",
        "      Purpose: Evaluates how well the model generalizes to unseen validation data.\n",
        "      '''\n",
        "      y_pred = self._predict(x_valid)\n",
        "      return accuracy_score(y_valid, y_pred)\n",
        "\n",
        "  # Predict Function\n",
        "  # --------------------------------------------------------------------------------\n",
        "  def _predict(self, x_test):\n",
        "      '''\n",
        "      Purpose: Predicts the classes of a list of a sentences.\n",
        "      '''\n",
        "      log_word_probs = np.dot(x_test, np.log(self.word_probs.T))\n",
        "      return np.argmax(log_word_probs, axis=1)\n",
        "\n",
        "  # Predict Report Function\n",
        "  # --------------------------------------------------------------------------------\n",
        "  def printReport(self, test):\n",
        "      '''\n",
        "      Purpose: Prints the performance report of the model that includes metrics such\n",
        "               as precision, recall, and f1-score.\n",
        "      '''\n",
        "      # Read the test data and load the sentences into a dataframe\n",
        "      test_data = pd.read_csv(test)\n",
        "      test_data.columns = ['Label', 'Sentence']\n",
        "      test_sentences = test_data['Sentence'].tolist()\n",
        "\n",
        "      # Transform test sentences using the model's vectorizer\n",
        "      x_test = self.vectorizer.transform(test_sentences).toarray()\n",
        "\n",
        "      # Extract the labels from the dataframe\n",
        "      y_test = test_data['Label'].values\n",
        "\n",
        "      # Get the predictions of the model\n",
        "      y_pred = self._predict(x_test)\n",
        "\n",
        "      # Print the performance report\n",
        "      print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0PokDz3MRte"
      },
      "source": [
        "> ### <u>Section 2: Performance Report</u>\n",
        "This code tests the model on the test dataset and returns an accuracy report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsBz0kAvMUf7",
        "outputId": "39acddfd-3089-45c4-d27d-e2901a81b7fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.85      0.87       625\n",
            "           1       0.87      0.80      0.83       625\n",
            "           2       0.85      0.90      0.88       625\n",
            "           3       0.92      0.86      0.89       625\n",
            "           4       0.85      0.84      0.85       625\n",
            "           5       0.81      0.92      0.86       625\n",
            "\n",
            "    accuracy                           0.86      3750\n",
            "   macro avg       0.87      0.86      0.86      3750\n",
            "weighted avg       0.87      0.86      0.86      3750\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "emotion_classifier = Naive_Bayes()\n",
        "\n",
        "# Train the model using the training and validation sets\n",
        "emotion_classifier.train(\"train.csv\", \"valid.csv\")\n",
        "\n",
        "# Print the performance report for the model\n",
        "emotion_classifier.printReport(\"test.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
